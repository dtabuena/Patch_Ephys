{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqFGLA3i8+Qp7UODwT3v3K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/Patch_Ephys/blob/main/Ephys_wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ephys_wrapper_local(dataset,VC_prot,IC_prot,strat_cols=['Cell_Type'],verbose=False, spike_args={'spike_thresh':20, 'high_dv_thresh': 50,'low_dv_thresh': -30,'window_ms': 2},manual_exclusions=[],age_bin_dict=None):\n",
        "    '''wrapper for single dataset pipeline'''\n",
        "    results = {}\n",
        "\n",
        "    '''Unpack'''\n",
        "    data_name = dataset['data_name']\n",
        "    data_source = dataset['data_source']\n",
        "    file_naming_scheme = dataset['file_naming_scheme']\n",
        "\n",
        "    ''' Gather and Catalog Source Data'''\n",
        "    abf_recordings_df, protocol_set = catalogue_recs(dataset['data_source'],\n",
        "                                                 dataset['file_naming_scheme'])\n",
        "\n",
        "    results['abf_recordings_df'] = abf_recordings_df\n",
        "    results['protocol_set'] = protocol_set\n",
        "\n",
        "\n",
        "    abf_recordings_df, _ = purge_wrong_clamp(abf_recordings_df,VC_prot,IC_prot)\n",
        "\n",
        "    results['abf_recordings_df'] = abf_recordings_df\n",
        "\n",
        "    csv_name = cell_prot_lut(abf_recordings_df,protocol_set,csv_name=data_name+'_Recording_LookUp')\n",
        "    results['prot_lut'] = csv_name\n",
        "\n",
        "    '''Set Internal Analysis Params'''\n",
        "    func_dict, arg_dict = init_func_arg_dicts()\n",
        "\n",
        "    '''Analyze Dataset'''\n",
        "    abf_recordings_df, problem_recs = analysis_iterator(abf_recordings_df,func_dict,arg_dict,verbose=verbose)\n",
        "    # clear_output(wait=True)\n",
        "    print('problem_recs')\n",
        "    _=[print('     '+r) for r in problem_recs]\n",
        "    results['problem_recs'] = problem_recs\n",
        "    results['abf_recordings_df'] = abf_recordings_df\n",
        "\n",
        "\n",
        "    '''Sort Cells'''\n",
        "    cell_df = cell_sorting(abf_recordings_df)\n",
        "    results['cell_df'] = cell_df\n",
        "\n",
        "    '''Consolidate to Cells'''\n",
        "    list_types = ['Recording_name','protocol','abf_timestamp', 'channelList']\n",
        "    any_types = [] + dataset['file_naming_scheme']\n",
        "    cell_df_con = cell_consolidation_v2(cell_df,list_types,any_types)\n",
        "    results['cell_df_con'] = cell_df_con\n",
        "\n",
        "    '''Simplify IV Data'''\n",
        "    cols_to_simplify = ['IV_Early', 'IV_Steady_State']\n",
        "    cell_df_nd = simplify_dicts(cell_df_con,cols_to_simplify)\n",
        "    results['cell_df_nd'] = cell_df_nd\n",
        "\n",
        "    '''Make Excell Friendly'''\n",
        "    keys_and_data_cols={'Stim_Levels_(pA)': ['Stim_Levels_(pA)', 'Spike_Counts' ],\n",
        "                    'IV_Early_(V_stim)': ['IV_Early_(V_stim)', 'IV_Early_(I_peak)', 'IV_Steady_State_(I_mean)']}\n",
        "    cell_df_csv = csv_frinedly(cell_df_nd,keys_and_data_cols)\n",
        "    results['cell_df_csv'] = cell_df_csv\n",
        "\n",
        "\n",
        "    ''' Convert to Current Density'''\n",
        "    size_col = 'Cmq_160.0'\n",
        "    current_col_list = ['IV_Early_(I_peak)_', 'IV_Steady_State_(I_mean)_']\n",
        "    cell_df_csv = current_density_correction(cell_df_csv, size_col, current_col_list)\n",
        "    results['cell_df_csv'] = cell_df_csv\n",
        "\n",
        "    '''Abridge DataFrame'''\n",
        "    abrg_exclusions = ['Recording_name',\n",
        "                    'protocol', 'abf_timestamp', 'channelList',  'Ra_10.0', 'Rm_10.0', 'tau_10.0', 'Cmq_10.0', 'Cmf_10.0',\n",
        "                    'Cmqf_10.0',  'Cmf_160.0', 'Cmqf_160.0', 'Cm_pc_160.0',\n",
        "                    'Gain_R2', 'Stim_Levels_(pA)', 'Spike_Counts',  'Gain_Vh',  'Vhold_spike',\n",
        "                        'Rin_Rsqr',  'Ramp_AP_thresh', 'Ramp_Vh', 'Ramp_Rheobase',\n",
        "                    'v_half','is_compensated','sum_delta'\n",
        "                    'IV_Early_(range)', 'IV_Early_(I_peak)', 'IV_Early_(I_mean)', 'IV_Early_(V_stim)', 'IV_Steady_State_(range)',\n",
        "                    'IV_Steady_State_(I_peak)', 'IV_Steady_State_(I_mean)', 'IV_Steady_State_(V_stim)', ]\n",
        "\n",
        "    abrg_keep = [c for c in cell_df_csv.columns if c not in abrg_exclusions]\n",
        "    cell_df_csv_abrg = cell_df_csv[abrg_keep]\n",
        "    results['cell_df_csv_abrg'] = cell_df_csv_abrg\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Add Age Bins\n",
        "    \"\"\"\n",
        "    if age_bin_dict is None:\n",
        "        age_bin_dict = {6:'<=6mo',\n",
        "                        7:'7-9mo',\n",
        "                        8:'7-9mo',\n",
        "                        9:'7-9mo',\n",
        "                        17:'17-19mo',\n",
        "                        18:'17-19mo',\n",
        "                        19:'17-19mo',}\n",
        "    cell_df_csv_abrg = convert_age_month_bins(cell_df_csv_abrg,age_bin_dict,age_key='age')\n",
        "\n",
        "    '''Stratify Cells By Type'''\n",
        "    # strat_df_dict = stratify_rec(cell_df_csv_abrg,strat_cols)\n",
        "    # strat_df_dict,_ = flatten_dict(strat_df_dict,{})\n",
        "    strat_df_dict = stratify_rec_v2(cell_df_csv_abrg,strat_cols)\n",
        "    write_strat_dfs_local(strat_df_dict, dataset['data_name']+'_results_stratified')\n",
        "    results['strat_df_dict'] = strat_df_dict\n",
        "    return results"
      ],
      "metadata": {
        "id": "aWetos08NPzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_frinedly(cell_df,keys_and_data_cols,remove_source = True):\n",
        "    cell_df_csv = cell_df.copy()\n",
        "\n",
        "    to_add = pd.DataFrame({cell_df.index.name : cell_df.index}).set_index(cell_df.index.name)\n",
        "\n",
        "    for k in keys_and_data_cols.keys():\n",
        "        for data_col in keys_and_data_cols[k]:\n",
        "            for cell in cell_df_csv.index:\n",
        "                label_value_list = cell_df_csv.loc[cell,k]\n",
        "                data_value_list = cell_df_csv.loc[cell,data_col]\n",
        "                if label_value_list is None: continue\n",
        "                label_value_len = len( label_value_list)\n",
        "                for i in range(label_value_len):\n",
        "                    val = int(cell_df_csv.loc[cell,k][i])\n",
        "                    str_val = str(val)\n",
        "                    str_val = format(val,\"=+04.0f\")\n",
        "                    new_col_name = data_col + '_' + str_val\n",
        "                    if new_col_name not in cell_df_csv.columns: cell_df_csv[new_col_name] = None\n",
        "                    to_add.at[cell,new_col_name] = data_value_list[i]\n",
        "\n",
        "    to_add = to_add.reindex(sorted(to_add.columns), axis=1)\n",
        "    cell_df_csv.update(to_add)\n",
        "    return cell_df_csv"
      ],
      "metadata": {
        "id": "iX1u6mUXR0P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_iterator(abf_recordings_df,func_dict,arg_dict,verbose=True):\n",
        "    problem_recs = []\n",
        "\n",
        "    for file_name in tqdm(abf_recordings_df.index):\n",
        "        abf = pyabf.ABF(file_name)\n",
        "        prot_name = abf.protocol\n",
        "        if verbose: print('\\n','     ',file_name)\n",
        "        if verbose: print('     ',prot_name)\n",
        "\n",
        "        # check for keyed protocol\n",
        "        if prot_name not in func_dict.keys():\n",
        "            # print('unknown protocol(func): ',  prot_name)\n",
        "            continue\n",
        "        if prot_name not in arg_dict.keys():\n",
        "            # print('unknown protocol(args): ',  prot_name)\n",
        "            continue\n",
        "\n",
        "        if not command_match(abf):\n",
        "            continue\n",
        "\n",
        "\n",
        "        analyzer_func = func_dict[prot_name]  # get analyzer from dict\n",
        "        args_for_analyzer =  [abf] + arg_dict[prot_name] # get args for analyzer from dict\n",
        "        # try:\n",
        "        results = analyzer_func(*args_for_analyzer) # run analyzer\n",
        "        # except:\n",
        "        #     print('\\n','error on: ' ,file_name)\n",
        "        #     print('analysis failed')\n",
        "        try:\n",
        "            for k in results.keys():\n",
        "                # New Col?\n",
        "                cols = abf_recordings_df.columns\n",
        "                if k not in cols:\n",
        "                    abf_recordings_df = init_col_object(abf_recordings_df,k)\n",
        "                abf_recordings_df.at[file_name,k] = results[k]\n",
        "        except:\n",
        "            print('\\n','error on: ' ,file_name)\n",
        "            print('recording results failed')\n",
        "            problem_recs.append(file_name)\n",
        "\n",
        "    return abf_recordings_df, problem_recs\n",
        "\n",
        "def init_col_object(df,name):\n",
        "        df[name] = None\n",
        "        df[name] = df[name].astype(object)\n",
        "        return df"
      ],
      "metadata": {
        "id": "bXrnQJxCR2In",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "c4bde55a-a20e-4952-9d75-698ab189c4d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'if' statement on line 18 (ipython-input-2-2888019125.py, line 20)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2-2888019125.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    analyzer_func = func_dict[prot_name]  # get analyzer from dict\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cell_sorting(abf_recordings_df):\n",
        "\n",
        "    unique_cells = list(set(abf_recordings_df['cell_id']))\n",
        "    unique_cells.sort()\n",
        "    transfer_cols = [c for c in abf_recordings_df.columns if 'cell_id' not in c]\n",
        "    cell_df = pd.DataFrame(index=list(unique_cells),columns = transfer_cols)\n",
        "\n",
        "\n",
        "    for cell in cell_df.index:\n",
        "        match = [cell in r for r in abf_recordings_df['cell_id']]\n",
        "        for col in transfer_cols:\n",
        "            match_values = list(abf_recordings_df[match][col])\n",
        "            # print('col', col)\n",
        "            # print(match_values)\n",
        "\n",
        "            cell_df.at[cell,col] = match_values\n",
        "    return cell_df"
      ],
      "metadata": {
        "id": "RVxn4xQzR46W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_loose(x):\n",
        "    if len(x)==0:\n",
        "        x_mean = []\n",
        "    if len(x)==1:\n",
        "        x_mean = x[0]\n",
        "    else:\n",
        "        x_mean = np.nanmean(x,0)\n",
        "    return x_mean"
      ],
      "metadata": {
        "id": "6mZUaHvdPJN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cell_consolidation(cell_df,list_types,any_types,average_types = True):\n",
        "    cell_df_con = cell_df.copy()\n",
        "    explicit_cols = ['IV_Early','IV_Steady_State','Stim_Levels_(pA)','Spike_Counts']\n",
        "\n",
        "    if average_types:\n",
        "        average_types = [c for c in cell_df_con.columns if c not in any_types and c not in list_types and c not in explicit_cols]\n",
        "\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        for col in list_types:\n",
        "            'do nothing, keep the list'\n",
        "        for col in any_types:\n",
        "            'they are all the same take the first'\n",
        "            cell_df_con.at[cell,col] = cell_df_con.at[cell,col][0]\n",
        "\n",
        "        for col in average_types:\n",
        "            multi_vals = cell_df_con.loc[cell,col]\n",
        "            try:\n",
        "                multi_vals = [v for v in multi_vals if v is not None]\n",
        "                single_val = mean_loose(multi_vals)\n",
        "                cell_df_con.at[cell,col] = single_val\n",
        "                # print(single_val)\n",
        "            except: 'Just keep going None'\n",
        "\n",
        "\n",
        "    # explicitly defined consolidations\n",
        "    for col in ['IV_Early', 'IV_Steady_State']:\n",
        "        # assert col in cell_df_con.index, f\"Column to consoidate not found: {col}\"\n",
        "        for cell in cell_df_con.index:\n",
        "            try:\n",
        "                multi_vals = cell_df_con.loc[cell,col]\n",
        "                multi_vals = consolidate_iv_recs(multi_vals)\n",
        "            except:\n",
        "                if np.isnan(multi_vals): multi_vals = None\n",
        "                else: multi_vals = 'ERROR'\n",
        "\n",
        "            if not isinstance(multi_vals, list): multi_vals=[multi_vals]\n",
        "            cell_df_con.at[cell,col] = multi_vals\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        multi_val_pair = (cell_df_con.loc[cell,'Stim_Levels_(pA)'], cell_df_con.loc[cell,'Spike_Counts'])\n",
        "        multi_val_pair = consolidate_gain_recs(multi_val_pair)\n",
        "\n",
        "        new_stim = multi_val_pair[0]\n",
        "        new_firing = multi_val_pair[1]\n",
        "        if len(new_stim)>0:\n",
        "            if isinstance(new_stim[0],list):\n",
        "                new_stim = new_stim[0]\n",
        "        if len(new_firing)>0:\n",
        "            if isinstance(new_firing[0],list):\n",
        "                new_firing = new_firing[0]\n",
        "\n",
        "        cell_df_con.at[cell,'Stim_Levels_(pA)'] = new_stim\n",
        "        cell_df_con.at[cell,'Spike_Counts'] = new_firing\n",
        "\n",
        "    # cell_df_con = cell_df_con.reindex(sorted(cell_df_con.columns), axis=1)\n",
        "    return cell_df_con\n"
      ],
      "metadata": {
        "id": "xtcgX-WPje0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_loose(x):\n",
        "    if len(x)==0:\n",
        "        x_mean = []\n",
        "    if len(x)==1:\n",
        "        x_mean = x[0]\n",
        "    else:\n",
        "        x_mean = np.nanmean(x,0)\n",
        "    return x_mean"
      ],
      "metadata": {
        "id": "U5Gtv7aelnhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cell_consolidation_v2(cell_df,list_types,any_types,average_types = True):\n",
        "    cell_df_con = cell_df.copy()\n",
        "    explicit_cols = ['IV_Early','IV_Steady_State','Stim_Levels_(pA)','Spike_Counts']\n",
        "    mem_fit_columns = ['Ra_10.0', 'Rm_10.0', 'tau_10.0', 'Cmq_10.0', 'Cmf_10.0',\n",
        "                    'Cmqf_10.0', 'Cm_pc_10.0', 'Ra_160.0', 'Rm_160.0', 'tau_160.0',\n",
        "                    'Cmq_160.0', 'Cmf_160.0', 'Cmqf_160.0', 'Cm_pc_160.0',]\n",
        "\n",
        "    if average_types:\n",
        "        average_types = [c for c in cell_df_con.columns if c not in any_types and c not in list_types and c not in explicit_cols and c not in mem_fit_columns]\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        cell_df_con.loc[cell] = consolidate_membrane_fit(cell_df_con.loc[cell])\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        for col in list_types:\n",
        "            'do nothing, keep the list'\n",
        "        for col in any_types:\n",
        "            'they are all the same take the first'\n",
        "            cell_df_con.at[cell,col] = cell_df_con.at[cell,col][0]\n",
        "\n",
        "        for col in average_types:\n",
        "            multi_vals = cell_df_con.loc[cell,col]\n",
        "            try:\n",
        "                multi_vals = [v for v in multi_vals if v is not None]\n",
        "                single_val = mean_loose(multi_vals)\n",
        "                cell_df_con.at[cell,col] = single_val\n",
        "                # print(single_val)\n",
        "            except: 'Just keep going None'\n",
        "\n",
        "\n",
        "    # explicitly defined consolidations\n",
        "    for col in ['IV_Early', 'IV_Steady_State']:\n",
        "        # assert col in cell_df_con.index, f\"Column to consoidate not found: {col}\"\n",
        "        for cell in cell_df_con.index:\n",
        "            try:\n",
        "                multi_vals = cell_df_con.loc[cell,col]\n",
        "                multi_vals = consolidate_iv_recs(multi_vals)\n",
        "            except:\n",
        "                if np.isnan(multi_vals): multi_vals = None\n",
        "                else: multi_vals = 'ERROR'\n",
        "\n",
        "            if not isinstance(multi_vals, list): multi_vals=[multi_vals]\n",
        "            cell_df_con.at[cell,col] = multi_vals\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        multi_val_pair = (cell_df_con.loc[cell,'Stim_Levels_(pA)'], cell_df_con.loc[cell,'Spike_Counts'])\n",
        "        multi_val_pair = consolidate_gain_recs(multi_val_pair)\n",
        "\n",
        "        new_stim = multi_val_pair[0]\n",
        "        new_firing = multi_val_pair[1]\n",
        "        if len(new_stim)>0:\n",
        "            if isinstance(new_stim[0],list):\n",
        "                new_stim = new_stim[0]\n",
        "        if len(new_firing)>0:\n",
        "            if isinstance(new_firing[0],list):\n",
        "                new_firing = new_firing[0]\n",
        "\n",
        "        cell_df_con.at[cell,'Stim_Levels_(pA)'] = new_stim\n",
        "        cell_df_con.at[cell,'Spike_Counts'] = new_firing\n",
        "\n",
        "    # cell_df_con = cell_df_con.reindex(sorted(cell_df_con.columns), axis=1)\n",
        "    return cell_df_con\n",
        "\n",
        "def cell_consolidation_v2(cell_df,list_types,any_types,average_types = True):\n",
        "    cell_df_con = cell_df.copy()\n",
        "    explicit_cols = ['IV_Early','IV_Steady_State','Stim_Levels_(pA)','Spike_Counts']\n",
        "    mem_fit_columns = ['Ra_10.0', 'Rm_10.0', 'tau_10.0', 'Cmq_10.0', 'Cmf_10.0',\n",
        "                    'Cmqf_10.0', 'Cm_pc_10.0', 'Ra_160.0', 'Rm_160.0', 'tau_160.0',\n",
        "                    'Cmq_160.0', 'Cmf_160.0', 'Cmqf_160.0', 'Cm_pc_160.0',]\n",
        "\n",
        "    if average_types:\n",
        "        average_types = [c for c in cell_df_con.columns if c not in any_types and c not in list_types and c not in explicit_cols and c not in mem_fit_columns]\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        cell_df_con.loc[cell] = consolidate_membrane_fit(cell_df_con.loc[cell])\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        for col in list_types:\n",
        "            'do nothing, keep the list'\n",
        "        for col in any_types:\n",
        "            'they are all the same take the first'\n",
        "            cell_df_con.at[cell,col] = cell_df_con.at[cell,col][0]\n",
        "\n",
        "        for col in average_types:\n",
        "            multi_vals = cell_df_con.loc[cell,col]\n",
        "            try:\n",
        "                multi_vals = [v for v in multi_vals if v is not None]\n",
        "                single_val = np.nanmean(multi_vals,0)\n",
        "                cell_df_con.at[cell,col] = single_val\n",
        "                # print(single_val)\n",
        "            except: 'Just keep going None'\n",
        "\n",
        "\n",
        "    # explicitly defined consolidations\n",
        "    for col in ['IV_Early', 'IV_Steady_State']:\n",
        "        # assert col in cell_df_con.index, f\"Column to consoidate not found: {col}\"\n",
        "        for cell in cell_df_con.index:\n",
        "            try:\n",
        "                multi_vals = cell_df_con.loc[cell,col]\n",
        "                multi_vals = consolidate_iv_recs(multi_vals)\n",
        "            except:\n",
        "                if np.isnan(multi_vals): multi_vals = None\n",
        "                else: multi_vals = 'ERROR'\n",
        "\n",
        "            if not isinstance(multi_vals, list): multi_vals=[multi_vals]\n",
        "            cell_df_con.at[cell,col] = multi_vals\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        multi_val_pair = (cell_df_con.loc[cell,'Stim_Levels_(pA)'], cell_df_con.loc[cell,'Spike_Counts'])\n",
        "        multi_val_pair = consolidate_gain_recs(multi_val_pair)\n",
        "\n",
        "        new_stim = multi_val_pair[0]\n",
        "        new_firing = multi_val_pair[1]\n",
        "        if len(new_stim)>0:\n",
        "            if isinstance(new_stim[0],list):\n",
        "                new_stim = new_stim[0]\n",
        "        if len(new_firing)>0:\n",
        "            if isinstance(new_firing[0],list):\n",
        "                new_firing = new_firing[0]\n",
        "\n",
        "        cell_df_con.at[cell,'Stim_Levels_(pA)'] = new_stim\n",
        "        cell_df_con.at[cell,'Spike_Counts'] = new_firing\n",
        "\n",
        "    # cell_df_con = cell_df_con.reindex(sorted(cell_df_con.columns), axis=1)\n",
        "    return cell_df_con\n",
        "\n",
        "def cell_consolidation_v2(cell_df,list_types,any_types,average_types = True):\n",
        "    cell_df_con = cell_df.copy()\n",
        "    explicit_cols = ['IV_Early','IV_Steady_State','Stim_Levels_(pA)','Spike_Counts']\n",
        "    mem_fit_columns = ['Ra_10.0', 'Rm_10.0', 'tau_10.0', 'Cmq_10.0', 'Cmf_10.0',\n",
        "                    'Cmqf_10.0', 'Cm_pc_10.0', 'Ra_160.0', 'Rm_160.0', 'tau_160.0',\n",
        "                    'Cmq_160.0', 'Cmf_160.0', 'Cmqf_160.0', 'Cm_pc_160.0',]\n",
        "\n",
        "    if average_types:\n",
        "        average_types = [c for c in cell_df_con.columns if c not in any_types and c not in list_types and c not in explicit_cols and c not in mem_fit_columns]\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        cell_df_con.loc[cell] = consolidate_membrane_fit(cell_df_con.loc[cell])\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        for col in list_types:\n",
        "            'do nothing, keep the list'\n",
        "        for col in any_types:\n",
        "            'they are all the same take the first'\n",
        "            cell_df_con.at[cell,col] = cell_df_con.at[cell,col][0]\n",
        "\n",
        "        for col in average_types:\n",
        "            multi_vals = cell_df_con.loc[cell,col]\n",
        "            try:\n",
        "                multi_vals = [v for v in multi_vals if v is not None]\n",
        "                single_val = np.nanmean(multi_vals,0)\n",
        "                cell_df_con.at[cell,col] = single_val\n",
        "                # print(single_val)\n",
        "            except: 'Just keep going None'\n",
        "\n",
        "\n",
        "    # explicitly defined consolidations\n",
        "    for col in ['IV_Early', 'IV_Steady_State']:\n",
        "        # assert col in cell_df_con.index, f\"Column to consoidate not found: {col}\"\n",
        "        for cell in cell_df_con.index:\n",
        "            try:\n",
        "                multi_vals = cell_df_con.loc[cell,col]\n",
        "                multi_vals = consolidate_iv_recs(multi_vals)\n",
        "            except:\n",
        "                if np.isnan(multi_vals): multi_vals = None\n",
        "                else: multi_vals = 'ERROR'\n",
        "\n",
        "            if not isinstance(multi_vals, list): multi_vals=[multi_vals]\n",
        "            cell_df_con.at[cell,col] = multi_vals\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        multi_val_pair = (cell_df_con.loc[cell,'Stim_Levels_(pA)'], cell_df_con.loc[cell,'Spike_Counts'])\n",
        "        multi_val_pair = consolidate_gain_recs(multi_val_pair)\n",
        "\n",
        "        new_stim = multi_val_pair[0]\n",
        "        new_firing = multi_val_pair[1]\n",
        "        if len(new_stim)>0:\n",
        "            if isinstance(new_stim[0],list):\n",
        "                new_stim = new_stim[0]\n",
        "        if len(new_firing)>0:\n",
        "            if isinstance(new_firing[0],list):\n",
        "                new_firing = new_firing[0]\n",
        "\n",
        "        cell_df_con.at[cell,'Stim_Levels_(pA)'] = new_stim\n",
        "        cell_df_con.at[cell,'Spike_Counts'] = new_firing\n",
        "\n",
        "    # cell_df_con = cell_df_con.reindex(sorted(cell_df_con.columns), axis=1)\n",
        "    return cell_df_con\n",
        "\n",
        "def consolidate_membrane_fit(cell):\n",
        "    cell_con=cell.copy()\n",
        "    columns=['Ra_10.0',\n",
        "            'Rm_10.0',\n",
        "            'tau_10.0',\n",
        "            'Cmq_10.0',\n",
        "            'Cmf_10.0',\n",
        "            'Cmqf_10.0',\n",
        "            'Cm_pc_10.0',\n",
        "            'Ra_160.0',\n",
        "            'Rm_160.0',\n",
        "            'tau_160.0',\n",
        "            'Cmq_160.0',\n",
        "            'Cmf_160.0',\n",
        "            'Cmqf_160.0',\n",
        "            'Cm_pc_160.0',]\n",
        "\n",
        "    cm = cell_con['Cmq_160.0']\n",
        "    ra = cell_con['Ra_160.0']\n",
        "    # rm = cell['Rm_160.0']\n",
        "\n",
        "\n",
        "    ra = [np.inf if n is None else n for n in ra]\n",
        "    # print('ra',ra)\n",
        "    ra_rank = np.argsort(np.argsort(-1*np.array(ra)))\n",
        "    # print('ra_rank',ra_rank)\n",
        "\n",
        "    cm = [-1 if n is None else n for n in cm]\n",
        "    # print('cm',cm)\n",
        "    cm_rank = np.argsort(np.argsort(cm)) #\n",
        "    # print('cm_rank',cm_rank)\n",
        "\n",
        "    rank = cm_rank + ra_rank\n",
        "\n",
        "    best_index = np.argsort(-rank)[0]\n",
        "    # print('best_index',best_index)\n",
        "    for c in columns:\n",
        "        cell_con.at[c] = cell_con.at[c][best_index]\n",
        "\n",
        "    return cell_con"
      ],
      "metadata": {
        "id": "fVZb4YzeR7Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_iv_recs(multi_vals):\n",
        "    multi_vals = [v for v in multi_vals if v is not None]\n",
        "    v_stim = [  mv['V_stim'] for mv in  multi_vals ]\n",
        "    peak_vals = [  mv['I_peak'] for mv in  multi_vals ]\n",
        "    if len(v_stim)>1:\n",
        "        rec_lengths = [len(v) for v in v_stim]\n",
        "        long_enough = np.where(np.array(rec_lengths) > 5)[0][0]\n",
        "        multi_vals = multi_vals[long_enough]\n",
        "        # print(multi_vals)\n",
        "    return multi_vals"
      ],
      "metadata": {
        "id": "2LoRN07nR9_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def simplify_dicts(cell_df,cols_to_simplify,remove_source = True):\n",
        "    cell_df_new = cell_df.copy()\n",
        "    for col in cols_to_simplify:\n",
        "        for cell in cell_df_new.index:\n",
        "            list_of_dicts = cell_df_new.loc[cell,col]\n",
        "            list_of_dicts = [d for d in list_of_dicts if d is not None]\n",
        "            if len(list_of_dicts) == 0: continue\n",
        "            # print(list_of_dicts)\n",
        "            list_of_keys = list(list_of_dicts[0].keys())\n",
        "            for k in list_of_keys:\n",
        "                vals_of_key = []\n",
        "                for i in range(len(list_of_dicts)):\n",
        "                    vals_of_key.append(  list_of_dicts[i][k] )\n",
        "                if len(vals_of_key) == 1: vals_of_key = vals_of_key[0]\n",
        "                new_col = col + '_(' + str(k) +')'\n",
        "                if new_col not in cell_df_new.columns:\n",
        "                    cell_df_new[new_col] = None\n",
        "                    cell_df_new[new_col] = cell_df_new[new_col].astype(object)\n",
        "                cell_df_new.at[cell,new_col] = vals_of_key\n",
        "        cell_df_new.drop(labels=col, axis = 1,inplace = True)\n",
        "    return cell_df_new"
      ],
      "metadata": {
        "id": "dC_Ts2jvR_zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_gain_recs(multi_val_pair):\n",
        "    min_stims = 5\n",
        "    mv_stim = multi_val_pair[0]\n",
        "    mv_fire = multi_val_pair[1]\n",
        "    mv_stim = [v.tolist() for v in mv_stim if v is not None]\n",
        "    mv_fire = [v.tolist() for v in mv_fire if v is not None]\n",
        "    results = (mv_stim, mv_fire)\n",
        "\n",
        "\n",
        "    if len(mv_stim)>1:\n",
        "        rec_lengths = [len(v) for v in mv_stim]\n",
        "        mv_stim = [v for v in mv_stim if len(v) >=min_stims]\n",
        "        mv_fire = [v for v in mv_fire if len(v) >=min_stims]\n",
        "\n",
        "    results = (mv_stim, mv_fire)\n",
        "\n",
        "    if len(mv_stim)>1:\n",
        "        stim_set = list(set( [vv for v in mv_stim for vv in v] ))# flat_list = [item for sublist in regular_list for item in sublist]\n",
        "        stim_set.sort()\n",
        "        new_vals_dict = {}\n",
        "        for s in stim_set:\n",
        "            matching_response =[]\n",
        "            matching_stim = []\n",
        "            for i in range(len(mv_stim)):\n",
        "                for j in range(len(mv_stim[i])):\n",
        "                    if mv_stim[i][j] == s:\n",
        "                        matching_stim.append(mv_stim[i][j])\n",
        "                        matching_response.append(mv_fire[i][j])\n",
        "            new_vals_dict[s] =  matching_response\n",
        "        new_stim_list = []\n",
        "        new_response_list = []\n",
        "        for k in new_vals_dict:\n",
        "            new_vals_dict[k] = np.mean(new_vals_dict[k])\n",
        "            new_stim_list.append(k)\n",
        "            new_response_list.append(new_vals_dict[k])\n",
        "\n",
        "\n",
        "        results = (new_stim_list, new_response_list)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "o-SIlwfESCYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def current_density_correction(cell_df,size_col,current_col_list,remove_old=True):\n",
        "    cell_df_cd = cell_df.copy()\n",
        "    ccl_exp = []\n",
        "    for ccl in current_col_list:\n",
        "        ccl_exp = ccl_exp + [c for c in cell_df.columns if ccl in c]\n",
        "    current_col_list = ccl_exp\n",
        "    for cell in cell_df.index:\n",
        "        size = cell_df.loc[cell,size_col]\n",
        "        for col in current_col_list:\n",
        "            try:\n",
        "                new_col = col +'_pApF'\n",
        "                cell_df_cd.at[ cell,new_col] = cell_df_cd.at[ cell,col] / size\n",
        "            except:\n",
        "                cell_df_cd.at[ cell,new_col] = None\n",
        "\n",
        "    cell_df_cd = cell_df_cd[[ c for c in cell_df_cd.columns if c not in current_col_list ]]\n",
        "\n",
        "    return cell_df_cd"
      ],
      "metadata": {
        "id": "OMOnzIYhSFiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratify_cells(cell_df,strat_col,xl_file_name='stratified_data.xlsx'):\n",
        "    types = list(set(cell_df[strat_col]))\n",
        "\n",
        "    new_dfs = {}\n",
        "    # options = {}\n",
        "    # options['strings_to_formulas'] = False\n",
        "    # options['strings_to_urls'] = False\n",
        "    writer = pd.ExcelWriter(xl_file_name) # , options=options\n",
        "    for t in types:\n",
        "        is_type = cell_df[strat_col] == t\n",
        "        new_dfs[t] = cell_df[is_type]\n",
        "        new_dfs[t].to_excel(writer, sheet_name=str(t))\n",
        "        # new_dfs[t].to_csv(str(t) + '_cell_df_csv.csv')\n",
        "        # files.download(str(t) + '_cell_df_csv.csv')\n",
        "    # writer.save()\n",
        "    writer.close()\n",
        "    files.download(xl_file_name)\n",
        "    return new_dfs"
      ],
      "metadata": {
        "id": "x5bFBjOISHEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratify_rec_v2(cell_df,strat_cols):\n",
        "    \"\"\" Takes in cell_df and splits it in to multiple dataframes stored in a\n",
        "    dict, where the key is the combonation of features definining an analysis\n",
        "    group and the value is a df containing the cells beloning to that group\"\"\"\n",
        "    # strat_cols = ['Cell_Type', 'Rec_date', 'Marker']\n",
        "    strat_keys = list()\n",
        "    for ind in cell_df.index:\n",
        "        strat_keys.append( '_'.join([cell_df.loc[ind,col] for col in strat_cols] ))\n",
        "    cell_df['strat_keys']=strat_keys\n",
        "    key_set = list(set(cell_df['strat_keys']))\n",
        "    strat_dict = {k:cell_df[cell_df['strat_keys']==k] for k in key_set}\n",
        "    return strat_dict"
      ],
      "metadata": {
        "id": "mkvUMw_rSIrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_dict(my_dict,flat_dict = {} ):\n",
        "    for k in my_dict.keys():\n",
        "        if isinstance(my_dict[k], dict):\n",
        "            sub_dict, sub_keys = flatten_dict(my_dict[k],flat_dict)\n",
        "            for sk in sub_keys:\n",
        "                flat_dict['_'+sk] = sub_dict[sk]\n",
        "        else:\n",
        "            flat_dict = my_dict\n",
        "    return flat_dict, list(flat_dict.keys())"
      ],
      "metadata": {
        "id": "oV4udxgUSKe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_strat_dfs(strat_dfs, xl_file_name='stratified_data.xlsx'):\n",
        "    if '.xlsx' not in xl_file_name: xl_file_name = xl_file_name+'.xlsx'\n",
        "    options = {}\n",
        "    # options['strings_to_formulas'] = False\n",
        "    # options['strings_to_urls'] = False\n",
        "    writer = pd.ExcelWriter(xl_file_name) # , options=options\n",
        "    for k in strat_dfs.keys():\n",
        "        cur_df = strat_dfs[k]\n",
        "        # cur_df = strat_dfs[k].T\n",
        "        cur_df.to_excel(writer, sheet_name=str(k))\n",
        "    # writer.save()\n",
        "    writer.close()\n",
        "    files.download(xl_file_name)\n",
        "    return None\n",
        "\n",
        "def write_strat_dfs_local(strat_dfs, xl_file_name='stratified_data.xlsx'):\n",
        "    if not xl_file_name.endswith('.xlsx'):\n",
        "        xl_file_name += '.xlsx'\n",
        "\n",
        "    # Check if the file exists and, if so, modify the file name\n",
        "    counter = 1\n",
        "    new_file_name = xl_file_name\n",
        "    while os.path.exists(new_file_name):\n",
        "        new_file_name = xl_file_name.replace('.xlsx', f'_C{counter}.xlsx')\n",
        "        counter += 1\n",
        "\n",
        "    # Save DataFrames to an Excel file with a unique name\n",
        "    with pd.ExcelWriter(new_file_name, engine='xlsxwriter') as writer:\n",
        "        for k, cur_df in strat_dfs.items():\n",
        "            cur_df.to_excel(writer, sheet_name=str(k))\n",
        "\n",
        "    return new_file_name  # Return the name of the created file for reference"
      ],
      "metadata": {
        "id": "5tV2iw9FSMaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restrat(strat_df_dict,alt_strat_groups  ):\n",
        "    alt_strat_dict = {}\n",
        "    for group in alt_strat_groups:\n",
        "        new_df = pd.DataFrame()\n",
        "        sorted_keys = sorted(list(strat_df_dict.keys()))\n",
        "        for k in sorted_keys:\n",
        "            v = strat_df_dict[k]\n",
        "        # for k,v in strat_df_dict.items():\n",
        "            if isinstance(group, list):\n",
        "                # print(group,'LIST')\n",
        "                ''' iter the list'''\n",
        "                group_name = group[0]\n",
        "                for sub_group in group:\n",
        "                    new_df = add_col( new_df,sub_group,k,v)\n",
        "            else:\n",
        "                group_name = group\n",
        "                new_df = add_col( new_df,group,k,v)\n",
        "        alt_strat_dict[group_name] = new_df\n",
        "    return alt_strat_dict\n",
        "\n",
        "\n",
        "def add_col( new_df,g,k,v):\n",
        "        new_col_name = g+'_'+k\n",
        "        clean_ser = v[g].reset_index().drop(labels='index',axis=1,inplace=False)\n",
        "        len_diff =  len(clean_ser)-len(new_df)\n",
        "        if len_diff>0:\n",
        "            blank_df = pd.DataFrame( index=range(len_diff),columns=new_df.columns)\n",
        "            new_df = pd.concat([new_df, blank_df],ignore_index=True)\n",
        "        new_df[new_col_name]=clean_ser\n",
        "        return new_df\n",
        "\n"
      ],
      "metadata": {
        "id": "sAoHjcYDdXSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def restratify_results(results_dict,labels,alt_strat_groups):\n",
        "    def add_col( new_df,g,k,v):\n",
        "        new_col_name = g+'_'+k\n",
        "        clean_ser = v[g].reset_index().drop(labels='index',axis=1,inplace=False)\n",
        "        len_diff =  len(clean_ser)-len(new_df)\n",
        "        if len_diff>0:\n",
        "            blank_df = pd.DataFrame( index=range(len_diff),columns=new_df.columns)\n",
        "            new_df = pd.concat([new_df, blank_df],ignore_index=True)\n",
        "        new_df[new_col_name]=clean_ser\n",
        "        return new_df\n",
        "\n",
        "    for k,v in results_dict.items():\n",
        "        v2 = v.assign(Strat_ID=[k]*len(v.index))\n",
        "        results_dict[k]=v2\n",
        "\n",
        "    alt_strat_dict = {}\n",
        "    for group in alt_strat_groups:\n",
        "        new_df = pd.DataFrame()\n",
        "        for k,v in results_dict.items():\n",
        "            if isinstance(group, list):\n",
        "                # print(group,'LIST')\n",
        "                ''' iter the list'''\n",
        "                group_name = group[0]\n",
        "                for sub_group in group:\n",
        "                    new_df = add_col( new_df,sub_group,k,v)\n",
        "            else:\n",
        "                group_name = group\n",
        "                new_df = add_col( new_df,group,k,v)\n",
        "        col_list = new_df.columns.tolist()\n",
        "        new_df = new_df[col_list]\n",
        "        alt_strat_dict[group_name] = new_df\n",
        "    return alt_strat_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2kSm67diDjyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_strat_dfs(strat_dfs, xl_file_name='stratified_data.xlsx'):\n",
        "    if '.xlsx' not in xl_file_name: xl_file_name = xl_file_name+'.xlsx'\n",
        "    # options = {}\n",
        "    # options['strings_to_formulas'] = False\n",
        "    # options['strings_to_urls'] = False\n",
        "    writer = pd.ExcelWriter(xl_file_name) # , options=options\n",
        "    for k in strat_dfs.keys():\n",
        "        cur_df = strat_dfs[k]\n",
        "\n",
        "        k=k.replace('/','_per_')\n",
        "        k=k[:31]\n",
        "        print(str(k))\n",
        "        cur_df.to_excel(writer, sheet_name=str(k))\n",
        "    # writer.save()\n",
        "    writer.close()\n",
        "    files.download(xl_file_name)\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "asPArmu2cn48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratify_response_curve(strat_df_dict,resp_curve_list,strat_list):\n",
        "    import re\n",
        "    response_curve_data = {}\n",
        "    for curve in resp_curve_list:\n",
        "        new_df_list = [pd.DataFrame() for l in strat_list]\n",
        "\n",
        "        for k,v in strat_df_dict.items():\n",
        "            for row in v.index:\n",
        "                cols = [c for c in v.columns if curve in c]\n",
        "                row_val = v.loc[row,cols]\n",
        "                strat_ind = [i for i in range(len(strat_list)) if strat_list[i] in v.loc[row,'Strat_ID']][0]\n",
        "                new_df_list[strat_ind] = pd.concat( [new_df_list[strat_ind],row_val] ,axis=1 )\n",
        "                response_curve_data[strat_list[strat_ind] +'_'+curve] = new_df_list[strat_ind]\n",
        "\n",
        "    for k,v in response_curve_data.items():\n",
        "        rows = v.index\n",
        "        # r_ints = [ float(re.findall(\"[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?\", r)[0]) for r in rows  ]\n",
        "        r_ints = [float(re.findall(r\"[-+]?[.]?\\d+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?\", r)[0]) for r in rows]\n",
        "        order = np.argsort(r_ints)\n",
        "        v = v.reindex(rows[order])\n",
        "        response_curve_data[k] = v\n",
        "\n",
        "    return response_curve_data"
      ],
      "metadata": {
        "id": "t-sBL_KWcmpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### reorg Files\n",
        "def strat_abfs_by_prot(new_dir, vm_local_dir,lut_df_loc):\n",
        "    lut_df = pd.read_csv(lut_df_loc)\n",
        "    try: shutil.rmtree('/content/'+new_dir)\n",
        "    except: None\n",
        "    os.mkdir('/content/'+new_dir)\n",
        "    for subdir, dirs, fils in os.walk(vm_local_dir):\n",
        "        for file in fils:\n",
        "            if 'abf' in file:\n",
        "                try:\n",
        "                    full_path = (os.path.join(subdir, file))\n",
        "                    abf_file = abf_or_name(full_path)\n",
        "                    prot_name = abf_file.protocol\n",
        "                    new_prot_dir = new_dir+'/'+prot_name\n",
        "                    try: os.mkdir('/content/'+new_prot_dir)\n",
        "                    except: None\n",
        "                    shutil.copyfile(full_path, new_prot_dir+'/'+file)\n",
        "                except: print('Failed', file)\n",
        "    shutil.make_archive(new_dir, 'zip', new_dir)\n",
        "    from google.colab import files\n",
        "    files.download(new_dir+'.zip')"
      ],
      "metadata": {
        "id": "NFlFJPvu5Fbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_qc(strat_df_dict,file_naming_scheme,qc_RR=.3,qc_AP_amp=50,qc_Rmp=-50,qc_Ra=50,manual_exclusions=[],exclusion_overide=[]):\n",
        "    fail_dict = {}\n",
        "    filtered_dict = strat_df_dict.copy()\n",
        "    for k in filtered_dict.keys():\n",
        "        data_df = filtered_dict[k].copy()\n",
        "\n",
        "\n",
        "        for r in data_df.index:\n",
        "            if r in exclusion_overide:\n",
        "                fail_dict[r] = f\"Exlusion Override\"\n",
        "                continue\n",
        "\n",
        "            if r in manual_exclusions:\n",
        "                fail_dict[r] = f\"Manual Exclusion\"\n",
        "                for c in data_df.columns:\n",
        "                    if c not in file_naming_scheme:\n",
        "                        data_df.at[r,c] = np.nan\n",
        "\n",
        "\n",
        "            Ra = data_df.loc[r,'Ra_160.0']\n",
        "            Rm = data_df.loc[r,'Rm_160.0']\n",
        "            RR = Ra/Rm\n",
        "            # print(RR,Ra,Rm)\n",
        "            if RR > qc_RR:\n",
        "                fail_dict[r] = f\"Fail - RR = {RR} > {qc_RR}\"\n",
        "                for c in data_df.columns:\n",
        "                    if c not in file_naming_scheme:\n",
        "                        data_df.at[r,c] = np.nan\n",
        "\n",
        "            if Ra > qc_Ra:\n",
        "                fail_dict[r] = f\"Fail - Ra = {Ra} > {qc_Ra}\"\n",
        "                for c in data_df.columns:\n",
        "                    if c not in file_naming_scheme:\n",
        "                        data_df.at[r,c] = np.nan\n",
        "\n",
        "            if data_df.loc[r,'ap_amplitutude'] < qc_AP_amp:\n",
        "                fail_dict[r] = f\"Fail - AP = {data_df.loc[r,'ap_amplitutude']} mV < {qc_AP_amp}mV\"\n",
        "                for c in data_df.columns:\n",
        "                    if c not in file_naming_scheme:\n",
        "                        data_df.at[r,c] = np.nan\n",
        "\n",
        "            if data_df.loc[r,'Rmp_mV'] > qc_Rmp:\n",
        "                fail_dict[r] = f\"Fail - Rmp = {data_df.loc[r,'Rmp_mV']} mV > {qc_Rmp} mV\"\n",
        "                for c in data_df.columns:\n",
        "                    if c not in file_naming_scheme:\n",
        "                        data_df.at[r,c] = np.nan\n",
        "        filtered_dict[k] = data_df\n",
        "    for k,v in fail_dict.items():\n",
        "        print(k , v)\n",
        "    return filtered_dict, fail_dict\n"
      ],
      "metadata": {
        "id": "P92jU5oZMNuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### alternate Stratification and QC params\n",
        "\n",
        "def analysis_consolidation(result_dict,\n",
        "                           manual_exclusions = [''],\n",
        "                           exclusion_overide = [''],\n",
        "                           single_val_strat_groups=['ap_amplitutude','Rmp_mV','Ra_160.0','Rm_160.0','Cm_pc_10.0','Cmq_160.0','Ra_160.0',\n",
        "                                                    'Rm_160.0','Gain_(HzpA)','max_adapt%','adapt_thresh_90','Rheobase','AP_thresh_US',\n",
        "                                                    'fast_after_hyperpol','Spike_latency_(ms)','Input_Resistance_MO'],\n",
        "                           resp_curve_list = ['IV_Early_(V_stim)','IV_Early_(I_peak)','IV_Steady_State_(I_mean)','Stim_Levels_(pA)','Spike_Counts'],\n",
        "                           file_naming_scheme = ['Rec_date', 'Genotype', 'Sex', 'Age', 'Slice_Num', 'Cell_num', 'Cell_Type'],\n",
        "                           QC_param = {}\n",
        "                           ):\n",
        "    print(QC_param)\n",
        "    QC_param_keys = QC_param.keys()\n",
        "    if 'qc_Rmp' not in QC_param_keys: QC_param['qc_Rmp']=-45\n",
        "    if 'qc_AP_amp' not in QC_param_keys: QC_param['qc_AP_amp']=40\n",
        "    if 'qc_RR' not in QC_param_keys: QC_param['qc_RR']=.35\n",
        "    if 'qc_Ra' not in QC_param_keys: QC_param['qc_Ra']=65\n",
        "    print(QC_param)\n",
        "\n",
        "    strat_df_dict = result_dict['strat_df_dict'].copy()\n",
        "    filtered_dict, fail_dict = final_qc(strat_df_dict,file_naming_scheme, QC_param['qc_RR'], QC_param['qc_Rmp'], QC_param['qc_AP_amp'],  QC_param['qc_Ra'],manual_exclusions=manual_exclusions,exclusion_overide=exclusion_overide)\n",
        "    alt_strat_dict = restratify_results(filtered_dict,file_naming_scheme,single_val_strat_groups)\n",
        "    response_curve_data = stratify_response_curve(filtered_dict,resp_curve_list,strat_list=[''])\n",
        "    alt_strat_dict.update(response_curve_data)\n",
        "    write_strat_dfs(alt_strat_dict, dataset['data_name']+'_results_stratified_alternate')\n",
        "\n",
        "    return {'filtered_dict':filtered_dict,\n",
        "            'fail_dict':fail_dict,\n",
        "            'alt_strat_dict':alt_strat_dict,\n",
        "            'response_curve_data':response_curve_data,\n",
        "            'single_val_strat_groups':single_val_strat_groups,\n",
        "            'resp_curve_list':resp_curve_list}\n"
      ],
      "metadata": {
        "id": "7EHKoHazwDfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### Make Some Bar Plots\n",
        "def summary_plots(alt_strat_dict,single_val_strat_groups):\n",
        "\n",
        "    for k,v in alt_strat_dict.items():\n",
        "        if k in single_val_strat_groups:\n",
        "            categs = list(v.columns)\n",
        "            fig,ax=plt.subplots(1,1, figsize = [ .3 + .5*len(categs), 2])\n",
        "            for ci in range(len(categs)):\n",
        "                c_vals = v[categs[ci]]\n",
        "                c_vals = [vi for vi in c_vals if np.isfinite(vi)]\n",
        "                c_mean = np.nanmean(c_vals)\n",
        "                c_sem = np.nanstd(c_vals)/np.sqrt(len(c_vals))\n",
        "                ax.bar(ci,c_mean,yerr=c_sem)\n",
        "                ax.scatter([ci]*len(c_vals),c_vals,color='k',marker='o')\n",
        "            ax.set_xticks(range(len(categs)))\n",
        "            categs_strats = [str(cat).split(\"__\")[1] for cat in categs]\n",
        "            ax.set_xticklabels(categs_strats,rotation =45)\n",
        "            ax.set_ylabel( str(categs[0]).split(\"__\")[0]  )\n",
        "    return None"
      ],
      "metadata": {
        "id": "jf7fhgm5ccDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def day_to_bin(day,age_bin_dict):\n",
        "\n",
        "    month = int(day) / 365 * 12\n",
        "    month_flr = int(month)\n",
        "    return age_bin_dict[month_flr],month_flr, month\n",
        "\n",
        "def convert_age_month_bins(cell_df,age_bin_dict,age_key='Age'):\n",
        "    cell_df['Age_Bin']=\"\"\n",
        "    for cell in cell_df.index:\n",
        "        age_day = cell_df.loc[cell,age_key]\n",
        "        age_day=age_day.replace('P',\"\")*1\n",
        "        age_bin,_,_ = day_to_bin(age_day,age_bin_dict)\n",
        "        cell_df.at[cell,'Age_Bin']=age_bin\n",
        "    return cell_df\n"
      ],
      "metadata": {
        "id": "NsFyNsLiOuvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def substitute_gain_rheobase(wrapper_results,rheo_gain_func_dict=None,rheo_gain_arg_dict=None,strat_cols=['Cell_Type'],age_bin_dict=None):\n",
        "    results = wrapper_results\n",
        "\n",
        "    rheo_df = wrapper_results['cell_df_csv_abrg']['Rheobase']\n",
        "    missing_rheo = [index for index, value in rheo_df.items() if np.isnan(value)]\n",
        "    print(missing_rheo)\n",
        "\n",
        "\n",
        "    prot_lut_df = pd.read_csv(wrapper_results['prot_lut'],index_col=0)\n",
        "    gain_files_to_use = prot_lut_df['IC - Gain - D10pA'][missing_rheo].values\n",
        "\n",
        "    abf_recordings_df = wrapper_results['abf_recordings_df']\n",
        "    abf_recordings_df_subset = abf_recordings_df[abf_recordings_df['Recording_name'].isin(gain_files_to_use)]\n",
        "\n",
        "    if rheo_gain_func_dict is None or rheo_gain_arg_dict is None:\n",
        "        spike_args_rheo={'spike_thresh':10, 'high_dv_thresh': 30,'low_dv_thresh': -10,'window_ms': 2}\n",
        "        rheo_gain_func_dict={}\n",
        "        rheo_gain_func_dict['IC - Gain - D10pA']= rheobase_analyzer\n",
        "        rheo_gain_arg_dict={}\n",
        "        rheo_gain_arg_dict['IC - Gain - D10pA']= [spike_args_rheo, True, False, False]\n",
        "\n",
        "    abf_recordings_df_subset, _ = analysis_iterator(abf_recordings_df_subset,rheo_gain_func_dict,rheo_gain_arg_dict)\n",
        "\n",
        "    abf_recordings_df.update(abf_recordings_df_subset)\n",
        "    results['abf_recordings_df'] = abf_recordings_df\n",
        "\n",
        "    '''Sort Cells'''\n",
        "    cell_df = cell_sorting(abf_recordings_df)\n",
        "    results['cell_df'] = cell_df\n",
        "\n",
        "    '''Consolidate to Cells'''\n",
        "    list_types = ['Recording_name','protocol','abf_timestamp', 'channelList']\n",
        "    any_types = [] + dataset['file_naming_scheme']\n",
        "    cell_df_con = cell_consolidation_v2(cell_df,list_types,any_types)\n",
        "    results['cell_df_con'] = cell_df_con\n",
        "\n",
        "    '''Simplify IV Data'''\n",
        "    cols_to_simplify = ['IV_Early', 'IV_Steady_State']\n",
        "    cell_df_nd = simplify_dicts(cell_df_con,cols_to_simplify)\n",
        "    results['cell_df_nd'] = cell_df_nd\n",
        "\n",
        "    '''Make Excell Friendly'''\n",
        "    keys_and_data_cols={'Stim_Levels_(pA)': ['Stim_Levels_(pA)', 'Spike_Counts' ],\n",
        "                    'IV_Early_(V_stim)': ['IV_Early_(V_stim)', 'IV_Early_(I_peak)', 'IV_Steady_State_(I_mean)']}\n",
        "    cell_df_csv = csv_frinedly(cell_df_nd,keys_and_data_cols)\n",
        "    results['cell_df_csv'] = cell_df_csv\n",
        "\n",
        "\n",
        "    ''' Convert to Current Density'''\n",
        "    size_col = 'Cmq_160.0'\n",
        "    current_col_list = ['IV_Early_(I_peak)_', 'IV_Steady_State_(I_mean)_']\n",
        "    cell_df_csv = current_density_correction(cell_df_csv, size_col, current_col_list)\n",
        "    results['cell_df_csv'] = cell_df_csv\n",
        "\n",
        "    '''Abridge DataFrame'''\n",
        "    abrg_exclusions = ['Recording_name',\n",
        "                    'protocol', 'abf_timestamp', 'channelList',  'Ra_10.0', 'Rm_10.0', 'tau_10.0', 'Cmq_10.0', 'Cmf_10.0',\n",
        "                    'Cmqf_10.0',  'Cmf_160.0', 'Cmqf_160.0', 'Cm_pc_160.0',\n",
        "                    'Gain_R2', 'Stim_Levels_(pA)', 'Spike_Counts',  'Gain_Vh',  'Vhold_spike',\n",
        "                        'Rin_Rsqr',  'Ramp_AP_thresh', 'Ramp_Vh', 'Ramp_Rheobase',\n",
        "                    'v_half','is_compensated','sum_delta'\n",
        "                    'IV_Early_(range)', 'IV_Early_(I_peak)', 'IV_Early_(I_mean)', 'IV_Early_(V_stim)', 'IV_Steady_State_(range)',\n",
        "                    'IV_Steady_State_(I_peak)', 'IV_Steady_State_(I_mean)', 'IV_Steady_State_(V_stim)', ]\n",
        "\n",
        "    abrg_keep = [c for c in cell_df_csv.columns if c not in abrg_exclusions]\n",
        "    cell_df_csv_abrg = cell_df_csv[abrg_keep]\n",
        "    results['cell_df_csv_abrg'] = cell_df_csv_abrg\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Add Age Bins\n",
        "    \"\"\"\n",
        "    if age_bin_dict is None:\n",
        "        age_bin_dict = {6:'<=6mo',\n",
        "                        7:'7-9mo',\n",
        "                        8:'7-9mo',\n",
        "                        9:'7-9mo',\n",
        "                        17:'17-19mo',\n",
        "                        18:'17-19mo',\n",
        "                        19:'17-19mo',}\n",
        "    cell_df_csv_abrg = convert_age_month_bins(cell_df_csv_abrg,age_bin_dict,age_key='Age')\n",
        "\n",
        "    '''Stratify Cells By Type'''\n",
        "    # strat_df_dict = stratify_rec(cell_df_csv_abrg,strat_cols)\n",
        "    # strat_df_dict,_ = flatten_dict(strat_df_dict,{})\n",
        "    strat_df_dict = stratify_rec_v2(cell_df_csv_abrg,strat_cols)\n",
        "    write_strat_dfs_local(strat_df_dict, dataset['data_name']+'_results_stratified')\n",
        "    results['strat_df_dict'] = strat_df_dict\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "wyfVDoOHfnki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}