{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ+MTRkwjaLRuUbJiNqJvp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/Patch_Ephys/blob/main/abf_handling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "10xyALme4JAP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def catalogue_recs(file_loc,cell_id_order):\n",
        "    'Read metadata from abf files stored in chosen folder and assigns'\n",
        "    'them to a dataframe for further processing. All further abf analyses'\n",
        "    'read files from this df and report values in the df.'\n",
        "\n",
        "    file_list = get_sub_files(file_loc)\n",
        "    # file_list = [file_loc+'/'+f for f in file_list]\n",
        "\n",
        "    file_list=[f for f in file_list if '.abf' in f]\n",
        "\n",
        "    abf_recordings_df = pd.DataFrame(data = file_list, columns=['file_name'])\n",
        "    abf_recordings_df = abf_recordings_df.set_index('file_name')\n",
        "\n",
        "    abf_recordings_df['Recording_name'] = None\n",
        "    abf_recordings_df['cell_id'] = None\n",
        "    for c in cell_id_order:\n",
        "        abf_recordings_df[c] = None\n",
        "\n",
        "    abf_recordings_df[\"protocol\"] = None\n",
        "    abf_recordings_df[\"abf_timestamp\"] = None\n",
        "    abf_recordings_df[\"channelList\"] = None\n",
        "\n",
        "\n",
        "    for r in np.arange(len(abf_recordings_df)):\n",
        "        try:\n",
        "            row_filename = abf_recordings_df.index[r]\n",
        "            if '.sta' in row_filename:\n",
        "                continue\n",
        "            base_name = os.path.basename(row_filename)\n",
        "            abf_recordings_df.loc[row_filename,'Recording_name'] = base_name\n",
        "            split_words = base_name.split('_')\n",
        "            re_code = ['_'+split_words[i] for i in range(len(cell_id_order))]\n",
        "            re_code = ''.join(re_code)[1:]\n",
        "            abf_recordings_df.loc[row_filename,'cell_id'] = re_code\n",
        "            for ci in range(len(cell_id_order)):\n",
        "                abf_recordings_df.loc[row_filename,cell_id_order[ci]] = split_words[ci]\n",
        "\n",
        "            abf = pyabf.ABF(row_filename)\n",
        "            abf_recordings_df.loc[row_filename,'protocol'] = abf.protocol\n",
        "            abf_recordings_df.at[row_filename,'channelList'] = abf.channelList\n",
        "            abf_recordings_df.at[row_filename,'abf_timestamp'] = abf.abfDateTimeString\n",
        "        except:\n",
        "            print(f'ERROR on :{row_filename}')\n",
        "    abf_recordings_df.sort_values('file_name',inplace=True)\n",
        "    protocol_set = list(set(abf_recordings_df['protocol']))\n",
        "    return abf_recordings_df, protocol_set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reorg_abfs(look_up_file, new_dir='reorg', local_abf_dir='/content/my_ephys_data'):\n",
        "    'Reorganize ABFs by clamp protocol'\n",
        "\n",
        "    lut_df = pd.read_csv('/content/Fast_Data_Recording_LookUp.csv')\n",
        "    try: shutil.rmtree('/content/'+new_dir)\n",
        "    except: None\n",
        "    os.mkdir('/content/'+new_dir)\n",
        "    for subdir, dirs, fils in os.walk(rootdir):\n",
        "        for file in fils:\n",
        "            full_path = (os.path.join(subdir, file))\n",
        "            abf_file = abf_or_name(full_path)\n",
        "            prot_name = abf_file.protocol\n",
        "            new_prot_dir = new_dir+'/'+prot_name\n",
        "            try: os.mkdir('/content/'+new_prot_dir)\n",
        "            except: None\n",
        "            shutil.copyfile(full_path, new_prot_dir+'/'+file)\n",
        "    shutil.make_archive(new_dir, 'zip', new_dir)\n",
        "    from google.colab import files\n",
        "    files.download(new_dir+'.zip')\n",
        "    return None"
      ],
      "metadata": {
        "id": "LLW008zT5Jgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sub_files(rootdir):\n",
        "    'Recursively search subfolders and return a list of all files'\n",
        "    file_list =[]\n",
        "    for rootdir, dirs, files in os.walk(rootdir):\n",
        "            file_list.extend([os.path.join(rootdir,f) for f in files])\n",
        "    return file_list\n"
      ],
      "metadata": {
        "id": "ANUqR2am5LND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cell_prot_lut(abf_recordings_df,protocol_set,csv_name='Protocol_LUT',download=True):\n",
        "    if '.csv' not in csv_name: csv_name =csv_name+'.csv'\n",
        "    file_index = list(abf_recordings_df.index)\n",
        "\n",
        "    cell_index = list(set(abf_recordings_df['cell_id']))\n",
        "\n",
        "    lut_df = pd.DataFrame(index=cell_index, columns=protocol_set).sort_index()\n",
        "\n",
        "\n",
        "    for f in file_index:\n",
        "        col_pos = abf_recordings_df.loc[f,'protocol']\n",
        "        row_pos = abf_recordings_df.loc[f,'cell_id']\n",
        "        lut_df.at[row_pos,col_pos] = os.path.basename(f)\n",
        "    lut_df.to_csv(csv_name)\n",
        "    # if download:\n",
        "    #     files.download(csv_name)\n",
        "    return csv_name"
      ],
      "metadata": {
        "id": "sQbNFgGA5NEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def daily_norm(control_df,test_df,focus_col,exclusions):\n",
        "    params_to_avg = [c for c in control_df.columns if c not in exclusions]\n",
        "\n",
        "    control_df_norm = control_df.copy()\n",
        "    test_df_norm = test_df.copy()\n",
        "\n",
        "    unique_col_vals = list(set(control_df_norm[focus_col]))\n",
        "    control_means = pd.DataFrame(index = unique_col_vals, columns = params_to_avg)\n",
        "    control_means\n",
        "\n",
        "    for ucv in unique_col_vals:\n",
        "        row_bool = [ucv == v for v in control_df_norm[focus_col] ]\n",
        "        for p in params_to_avg:\n",
        "            try:\n",
        "                matching_param_vals = control_df_norm.loc[row_bool,p]\n",
        "                matching_param_vals = [v for v in matching_param_vals if v is not None]\n",
        "                control_means.at[ucv,p] = np.nanmean(matching_param_vals).item()\n",
        "            except:\n",
        "                print('Error: ',ucv,p)\n",
        "                print(matching_param_vals)\n",
        "                print('Error: ',ucv,p)\n",
        "    try:\n",
        "        for cc in control_df_norm.index:\n",
        "            for p in params_to_avg:\n",
        "                norm_bin = control_df_norm.loc[cc,focus_col]\n",
        "                norm_base_val = control_means.loc[norm_bin,p]\n",
        "                current_val = control_df_norm.loc[cc,p]\n",
        "                if norm_base_val == 0:\n",
        "                   control_df_norm.at[cc,p] = 'zero baseline'\n",
        "                else:\n",
        "                    control_df_norm.at[cc,p] = control_df_norm.loc[cc,p] / norm_base_val\n",
        "        for ct in control_df_norm.index:\n",
        "            for p in params_to_avg:\n",
        "                norm_bin = control_df_norm.loc[ct,focus_col]\n",
        "                norm_base_val = control_means.loc[norm_bin,p]\n",
        "                current_val = test_df.loc[ct,p]\n",
        "                test_df.at[ct,p] = test_df.loc[ct,p] / norm_base_val\n",
        "                if norm_base_val == 0:\n",
        "                   test_df.at[cc,p] = 'zero baseline'\n",
        "                else:\n",
        "                    test_df.at[cc,p] = test_df.loc[cc,p] / norm_base_val\n",
        "    except:\n",
        "        try: print(cc)\n",
        "        except: None\n",
        "        try: print(ct)\n",
        "        except: None\n",
        "        try: print(p)\n",
        "        except: None\n",
        "        try: print(norm_base_val)\n",
        "        except: None\n",
        "        try: print(current_val)\n",
        "        except: None\n",
        "    return control_df_norm, test_df, control_means\n",
        "\n",
        "\n",
        "\n",
        "# exclusions = ['Recording_name', 'Rec_date', 'Virus', 'GenoType', 'Sex',\n",
        "#               'Age', 'Slice_Num', 'Cell_num', 'Cell_Type', 'protocol',\n",
        "#               'abf_timestamp', 'channelList', 'Stim_Levels_(pA)', 'Spike_Counts',\n",
        "#               'IV_Steady_State_(V_stim)', 'IV_Steady_State_(I_peak)', 'IV_Steady_State_(I_mean)',\n",
        "#               'IV_Early_(range)','IV_Early_(I_peak)','IV_Early_(I_mean)','IV_Early_(V_stim)','IV_Steady_State_(range)' ]\n",
        "\n",
        "# control_df = new_dfs['CA3xNEG']\n",
        "# test_df = new_dfs['CA3xPOS']\n",
        "# focus_col = 'Rec_date'\n",
        "\n",
        "# _,_,control_means = daily_norm(control_df,test_df,focus_col,exclusions)\n",
        "\n",
        "# display(control_means)"
      ],
      "metadata": {
        "id": "7Atq56Ol5PGT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}